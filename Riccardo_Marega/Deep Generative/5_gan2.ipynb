{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNIHXItmNORkFPoHOM+nolB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"8RRFUN5R3-RM","colab":{"base_uri":"https://localhost:8080/","height":495},"executionInfo":{"status":"error","timestamp":1744299429194,"user_tz":-120,"elapsed":2275123,"user":{"displayName":"Piergiuseppe Liuzzi","userId":"11508690843381073106"}},"outputId":"160dd8ed-20d9-444f-d202-7a48727a390f"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 26.4M/26.4M [00:02<00:00, 11.7MB/s]\n","100%|██████████| 29.5k/29.5k [00:00<00:00, 194kB/s]\n","100%|██████████| 4.42M/4.42M [00:01<00:00, 3.70MB/s]\n","100%|██████████| 5.15k/5.15k [00:00<00:00, 10.2MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/30 - Generator loss: 1.9982, Discriminator loss: 0.5149\n","Epoch 2/30 - Generator loss: 2.1473, Discriminator loss: 0.5013\n","Epoch 3/30 - Generator loss: 1.9226, Discriminator loss: 0.5712\n","Epoch 4/30 - Generator loss: 1.6871, Discriminator loss: 0.6689\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-8eb1444dfd69>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Fake images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mfake_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# G(z), detach to avoid gradient to G when training D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mout_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_imgs\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# D(fake)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mloss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_fake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-8eb1444dfd69>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# First upsampling convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvT1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         )\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m         return F.conv_transpose2d(\n\u001b[0m\u001b[1;32m   1163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms, utils\n","import numpy as np\n","import os\n","\n","# Hyperparameters\n","z_dim = 100          # Dimension of generator input (noise vector)\n","g_feat = 128         # Base number of generator feature maps\n","d_feat = 64          # Base number of discriminator feature maps\n","batch_size = 128\n","learning_rate = 2e-4\n","epochs = 30\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Load Fashion-MNIST dataset (normalized to [0,1])\n","transform = transforms.ToTensor()\n","train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Define the Generator network\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        # We will project the random z into a 7x7 feature map with g_feat*2 channels, then upsample to 28x28\n","        self.fc = nn.Linear(z_dim, g_feat*2 * 7 * 7)\n","        # Transposed convolutions to scale up the image\n","        self.convT1 = nn.ConvTranspose2d(g_feat*2, g_feat, kernel_size=4, stride=2, padding=1)  # 7x7 -> 14x14\n","        self.convT2 = nn.ConvTranspose2d(g_feat, 1, kernel_size=4, stride=2, padding=1)         # 14x14 -> 28x28\n","        # Batch norms (except for output layer)\n","        self.bn1 = nn.BatchNorm2d(g_feat*2)\n","        self.bn2 = nn.BatchNorm2d(g_feat)\n","\n","    def forward(self, z):\n","        # Fully connected to reshape into [batch, g_feat*2, 7, 7]\n","        x = self.fc(z)\n","        x = self.bn1(x.view(-1, g_feat*2, 7, 7))\n","        x = F.relu(x)\n","        # First upsampling convolution\n","        x = self.convT1(x)\n","        x = self.bn2(x)\n","        x = F.relu(x)\n","        # Second upsampling convolution to get 1x28x28 image\n","        x = torch.sigmoid(self.convT2(x))  # Sigmoid to output pixels in [0,1]\n","        return x\n","\n","# Define the Discriminator network\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        # Convolution layers to downsample the image\n","        self.conv1 = nn.Conv2d(1, d_feat, kernel_size=4, stride=2, padding=1)    # 28x28 -> 14x14\n","        self.conv2 = nn.Conv2d(d_feat, d_feat*2, kernel_size=4, stride=2, padding=1)  # 14x14 -> 7x7\n","        self.fc = nn.Linear(d_feat*2 * 7 * 7, 1)  # Fully connected to single logit output\n","        # Batch norm (not applied to input layer conv1)\n","        self.bn2 = nn.BatchNorm2d(d_feat*2)\n","\n","    def forward(self, x):\n","        # Downsample with LeakyReLU activations\n","        x = F.leaky_relu(self.conv1(x), negative_slope=0.2)\n","        x = F.leaky_relu(self.bn2(self.conv2(x)), negative_slope=0.2)\n","        x = x.view(x.size(0), -1)  # flatten\n","        # Output layer (we will apply Sigmoid after obtaining this logit)\n","        logit = self.fc(x)\n","        out = torch.sigmoid(logit)  # probability of being real\n","        return out\n","\n","# Initialize generator, discriminator, and optimizers\n","G = Generator().to(device)\n","D = Discriminator().to(device)\n","opt_G = optim.Adam(G.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n","opt_D = optim.Adam(D.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n","criterion = nn.BCELoss()\n","\n","os.makedirs('gan_outputs', exist_ok=True)\n","fixed_noise = torch.randn(16, z_dim, device=device)  # fixed noise for monitoring progress\n","\n","for epoch in range(1, epochs+1):\n","    G.train(); D.train()\n","    g_loss_sum = 0.0\n","    d_loss_sum = 0.0\n","    for real_imgs, _ in train_loader:\n","        real_imgs = real_imgs.to(device)\n","        batch_size_curr = real_imgs.size(0)\n","\n","        # Labels for real and fake\n","        real_labels = torch.ones(batch_size_curr, device=device)\n","        fake_labels = torch.zeros(batch_size_curr, device=device)\n","\n","        # ---- Train Discriminator ----\n","        # Real images\n","        opt_D.zero_grad()\n","        out_real = D(real_imgs)               # D(real)\n","        loss_real = criterion(out_real.squeeze(), real_labels)\n","        # Fake images\n","        z = torch.randn(batch_size_curr, z_dim, device=device)\n","        fake_imgs = G(z).detach()             # G(z), detach to avoid gradient to G when training D\n","        out_fake = D(fake_imgs)              # D(fake)\n","        loss_fake = criterion(out_fake.squeeze(), fake_labels)\n","        # Total discriminator loss\n","        d_loss = loss_real + loss_fake\n","        d_loss.backward()\n","        opt_D.step()\n","\n","        # ---- Train Generator ----\n","        opt_G.zero_grad()\n","        z = torch.randn(batch_size_curr, z_dim, device=device)\n","        fake_imgs = G(z)                     # new fake images for generator update\n","        out_fake = D(fake_imgs)              # D's opinion on these fakes\n","        # Generator tries to fool discriminator: we want D(fake) to output 1 (real)\n","        g_loss = criterion(out_fake.squeeze(), real_labels)\n","        g_loss.backward()\n","        opt_G.step()\n","\n","        # Accumulate losses for monitoring\n","        g_loss_sum += g_loss.item()\n","        d_loss_sum += d_loss.item()\n","    # End of epoch\n","    avg_g_loss = g_loss_sum / len(train_loader)\n","    avg_d_loss = d_loss_sum / len(train_loader)\n","    print(f\"Epoch {epoch}/{epochs} - Generator loss: {avg_g_loss:.4f}, Discriminator loss: {avg_d_loss:.4f}\")\n","\n","    # Save generator outputs on fixed_noise to monitor training progress\n","    G.eval()\n","    with torch.no_grad():\n","        fake_samples = G(fixed_noise).cpu()\n","    utils.save_image(fake_samples, f\"gan_outputs/fixed_samples_epoch{epoch}.png\", nrow=4, normalize=True)\n","\n","# After training, save a larger batch of generated images and an interpolation\n","G.eval()\n","with torch.no_grad():\n","    # Generate 64 random images\n","    z = torch.randn(64, z_dim, device=device)\n","    fake_images = G(z).cpu()\n","    utils.save_image(fake_images, \"gan_outputs/generated_images.png\", nrow=8, normalize=True)\n","    # Interpolate between two random latent vectors\n","    z_start = torch.randn(1, z_dim, device=device)\n","    z_end   = torch.randn(1, z_dim, device=device)\n","    # Generate interpolation series of 10 images\n","    alphas = torch.linspace(0, 1, steps=10).to(device)\n","    interp_images = []\n","    for alpha in alphas:\n","        z_interp = (1 - alpha) * z_start + alpha * z_end\n","        interp_img = G(z_interp).cpu()\n","        interp_images.append(interp_img)\n","    interp_images = torch.cat(interp_images, dim=0)\n","    utils.save_image(interp_images, \"gan_outputs/interpolation.png\", nrow=10, normalize=True)\n","print(\"GAN training complete. Check 'gan_outputs/' for generated images.\")\n"]}]}