{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNbqsz3dvJj1NH+owShzT5G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"m61UgTrJNVTC","colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"status":"error","timestamp":1744300127237,"user_tz":-120,"elapsed":139129,"user":{"displayName":"Piergiuseppe Liuzzi","userId":"11508690843381073106"}},"outputId":"68d1691a-131e-44c4-f4eb-89f00ba80a55"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch 1 [0/60000] Loss: 0.0097\n","Train Epoch 1 [2560/60000] Loss: 0.0047\n","Train Epoch 1 [5120/60000] Loss: 0.0016\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-89553acd0c8b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreal_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, utils\n","import math, os\n","\n","# Hyperparameters\n","n_steps = 1000        # Number of diffusion timesteps (T)\n","beta_start = 1e-4\n","beta_end = 0.02\n","batch_size = 128\n","learning_rate = 2e-4\n","epochs = 50            # Note: more epochs (e.g., 50+) are typically needed for good results\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Prepare Fashion-MNIST data\n","transform = transforms.ToTensor()\n","train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Create diffusion process constants (beta, alpha, etc.)\n","betas = torch.linspace(beta_start, beta_end, n_steps).to(device)            # schedule from 1e-4 to 0.02\n","alphas = 1.0 - betas\n","alpha_bars = torch.cumprod(alphas, dim=0).to(device)                        # cumulative product \\bar{\\alpha}_t\n","\n","# Sinusoidal position embedding for timesteps (like in Transformer, or use nn.Embedding)\n","def sinusoidal_time_embedding(timesteps, embed_dim=256):\n","    \"\"\"\n","    Create sinusoidal timestep embeddings (batch_size x embed_dim) for given timesteps.\n","    \"\"\"\n","    # Timesteps shape: (batch,)\n","    half_dim = embed_dim // 2\n","    # Compute sinusoidal frequencies\n","    freq = torch.exp(-math.log(10000) * torch.arange(0, half_dim, device=device) / half_dim)\n","    # Outer product: (batch_size, half_dim)\n","    angles = timesteps[:, None].float() * freq[None, :]\n","    emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=1)\n","    return emb  # shape: (batch, embed_dim)\n","\n","# Define the U-Net like model for epsilon_theta(x_t, t)\n","class DiffusionModel(nn.Module):\n","    def __init__(self):\n","        super(DiffusionModel, self).__init__()\n","        self.embed_dim = 256\n","        # Define convolutional layers for downsampling\n","        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)         # 28x28 -> 28x28 (no downsample yet)\n","        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)  # 28x28 -> 14x14\n","        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1) # 14x14 -> 7x7\n","        # Bottleneck convolution\n","        self.conv4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)      # 7x7 -> 7x7\n","        # Define transposed conv layers for upsampling\n","        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1) # 7x7 -> 14x14\n","        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)  # 14x14 -> 28x28\n","        # Final output conv (predict noise)\n","        self.conv_out = nn.Conv2d(64, 1, kernel_size=3, padding=1)\n","        # Embedding transform layers for time conditioning (to match channels)\n","        self.time_fc1 = nn.Linear(self.embed_dim, 64)\n","        self.time_fc2 = nn.Linear(self.embed_dim, 128)\n","        self.time_fc3 = nn.Linear(self.embed_dim, 256)\n","        self.time_fc4 = nn.Linear(self.embed_dim, 256)\n","        self.time_fc5 = nn.Linear(self.embed_dim, 128)\n","        self.time_fc6 = nn.Linear(self.embed_dim, 64)\n","\n","    def forward(self, x, t):\n","        # x shape: [batch, 1, 28, 28], t shape: [batch] (timestep indices)\n","        # Obtain time embeddings for t\n","        # We'll use sinusoidal embedding; alternatively, could use nn.Embedding learned embeddings\n","        time_emb = sinusoidal_time_embedding(t, self.embed_dim)  # shape: (batch, embed_dim)\n","        # Map time embedding to each required channel dimension\n","        emb1 = self.time_fc1(time_emb)[:, :, None, None]    # to shape (batch, 64, 1, 1)\n","        emb2 = self.time_fc2(time_emb)[:, :, None, None]    # to (batch, 128, 1, 1)\n","        emb3 = self.time_fc3(time_emb)[:, :, None, None]    # to (batch, 256, 1, 1)\n","        emb4 = self.time_fc4(time_emb)[:, :, None, None]    # to (batch, 256, 1, 1)\n","        emb5 = self.time_fc5(time_emb)[:, :, None, None]    # to (batch, 128, 1, 1)\n","        emb6 = self.time_fc6(time_emb)[:, :, None, None]    # to (batch, 64, 1, 1)\n","\n","        # Downward (encoding) path with time conditioning\n","        h1 = F.relu(self.conv1(x) + emb1)            # add time embed to first conv output (broadcasted)\n","        h2 = F.relu(self.conv2(h1) + emb2)           # downsample to 14x14, add time embedding\n","        h3 = F.relu(self.conv3(h2) + emb3)           # downsample to 7x7, add time embedding\n","        h4 = F.relu(self.conv4(h3) + emb4)           # bottleneck conv at 7x7, add time embedding\n","        # Upward (decoding) path with skip connections\n","        u1 = F.relu(self.deconv1(h4) + emb5)         # upsample to 14x14, add time embedding\n","        # Skip connection from h2 (14x14 feature map)\n","        u1 = u1 + h2                                 # (both u1 and h2 have 128 channels)\n","        u2 = F.relu(self.deconv2(u1) + emb6)         # upsample to 28x28, add time embedding\n","        # Skip connection from h1 (28x28 feature map)\n","        u2 = u2 + h1                                 # (both u2 and h1 have 64 channels)\n","        out = self.conv_out(u2)                      # output noise prediction (no activation here)\n","        return out\n","\n","# Initialize model and optimizer\n","model = DiffusionModel().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","os.makedirs('ddpm_outputs', exist_ok=True)\n","\n","# Training loop\n","for epoch in range(1, 3):#epochs+1):\n","    model.train()\n","    epoch_loss = 0.0\n","    for batch_idx, (real_imgs, _) in enumerate(train_loader):\n","        # print(batch_idx)\n","        real_imgs = real_imgs.to(device)\n","        # Sample random timesteps for each image in the batch\n","        t = torch.randint(0, n_steps, (real_imgs.size(0),), device=device).long()\n","        # Sample random noise\n","        noise = torch.randn_like(real_imgs)\n","        # Compute x_t from x_0 (real image) and noise: x_t = sqrt(alpha_bar[t])*x0 + sqrt(1-alpha_bar[t])*noise\n","        alpha_bar_t = alpha_bars[t].view(-1, 1, 1, 1)       # shape (batch,1,1,1)\n","        noisy_imgs = torch.sqrt(alpha_bar_t) * real_imgs + torch.sqrt(1 - alpha_bar_t) * noise\n","        # Predict the noise using the model\n","        pred_noise = model(noisy_imgs, t)\n","        # Loss: MSE between the predicted noise and true noise\n","        loss = F.mse_loss(pred_noise, noise)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item() * real_imgs.size(0)\n","        if batch_idx % 20 == 0:\n","            print(f\"Train Epoch {epoch} [{batch_idx*len(real_imgs)}/{len(train_loader.dataset)}] Loss: {loss.item()/len(real_imgs):.4f}\")\n","    avg_loss = epoch_loss / len(train_loader.dataset)\n","    print(f\"Epoch {epoch}/{epochs} - Training loss (MSE): {avg_loss:.6f}\")\n","    # (Optional) save model or generate samples at intermediate epochs for monitoring\n","    # We will generate final samples after training below.\n","#%%\n","# Sampling loop (generate new images from the model)\n","model.eval()\n","with torch.no_grad():\n","    num_samples = 16\n","    # Start from pure noise\n","    x_t = torch.randn(num_samples, 1, 28, 28, device=device)  # x_T\n","    for t in range(n_steps-1, -1, -1):  # from T-1 down to 0\n","        t_batch = torch.tensor([t] * num_samples, device=device)\n","        # Predict noise at this timestep\n","        pred_noise = model(x_t, t_batch)\n","        # Compute parameters for reverse update\n","        alpha_t = alphas[t]\n","        alpha_bar_t = alpha_bars[t]\n","        # Formula for the mean of p(x_{t-1} | x_t):\n","        # (1/sqrt(alpha_t)) * (x_t - (1 - alpha_t)/sqrt(1 - alpha_bar_t) * pred_noise)\n","        mean = (1.0 / torch.sqrt(alpha_t)) * (x_t - (1 - alpha_t) / torch.sqrt(1 - alpha_bar_t) * pred_noise)\n","        if t > 0:\n","            # Sample z ~ N(0, I)\n","            z = torch.randn_like(x_t)\n","            sigma_t = torch.sqrt(betas[t])\n","            x_t = mean + sigma_t * z  # add noise for stochasticity\n","        else:\n","            x_t = mean  # at last step, no noise added\n","    generated = x_t.cpu()\n","# Save generated samples as an image grid\n","utils.save_image(generated, \"ddpm_outputs/sample_generation.png\", nrow=2)\n","print(\"DDPM sampling complete. Generated samples saved to 'ddpm_outputs/sample_generation.png'.\")"]}]}