{"cells":[{"cell_type":"markdown","id":"81ba0e95","metadata":{"id":"81ba0e95"},"source":["# ðŸŽ“ VAE Tutorial on Fashion-MNIST\n","\n","This notebook demonstrates how to train a Variational Autoencoder (VAE) on the Fashion-MNIST dataset using PyTorch. Youâ€™ll learn how the model works, how to train it, and how to visualize the latent space."]},{"cell_type":"code","execution_count":null,"id":"8cf4a1f2","metadata":{"id":"8cf4a1f2"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms, utils\n","import matplotlib.pyplot as plt\n","import os"]},{"cell_type":"markdown","id":"c0377477","metadata":{"id":"c0377477"},"source":["## ðŸ”§ Step 1: Define Hyperparameters and Device"]},{"cell_type":"code","execution_count":null,"id":"cfae7ac0","metadata":{"id":"cfae7ac0"},"outputs":[],"source":["# Hyperparameters\n","latent_dim = 2        # Dimensionality of latent space (2 for visualization)\n","hidden_dim = 500      # Hidden layer size for encoder and decoder MLP\n","batch_size = 128\n","learning_rate = 1e-3\n","epochs = 20\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","id":"d071f02b","metadata":{"id":"d071f02b"},"source":["## ðŸ“¦ Step 2: Load the Dataset"]},{"cell_type":"code","execution_count":null,"id":"0201bded","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0201bded","executionInfo":{"status":"ok","timestamp":1744298471656,"user_tz":-120,"elapsed":6669,"user":{"displayName":"Piergiuseppe Liuzzi","userId":"11508690843381073106"}},"outputId":"b69dacee-5d76-490e-f88e-70d648a1645b"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.4M/26.4M [00:02<00:00, 12.3MB/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.5k/29.5k [00:00<00:00, 210kB/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.42M/4.42M [00:01<00:00, 3.87MB/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.15k/5.15k [00:00<00:00, 12.0MB/s]\n"]}],"source":["# Load Fashion-MNIST dataset\n","transform = transforms.ToTensor()  # Converts images to [0,1] range tensors\n","train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n","test_dataset  = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"markdown","id":"70cb0531","metadata":{"id":"70cb0531"},"source":["## ðŸ§  Step 3: Define the VAE Model"]},{"cell_type":"code","execution_count":null,"id":"a0a1dd25","metadata":{"id":"a0a1dd25"},"outputs":[],"source":["# VAE Model Definition\n","class VAE(nn.Module):\n","    def __init__(self):\n","        super(VAE, self).__init__()\n","        # Encoder layers\n","        self.fc1 = nn.Linear(28*28, hidden_dim)\n","        self.fc_mu = nn.Linear(hidden_dim, latent_dim)      # outputs mean Î¼\n","        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)  # outputs log(variance)\n","        # Decoder layers\n","        self.fc_dec1 = nn.Linear(latent_dim, hidden_dim)\n","        self.fc_dec2 = nn.Linear(hidden_dim, 28*28)\n","\n","    def encode(self, x):\n","        \"\"\"Encode image x into latent parameters (mu, logvar).\"\"\"\n","        h = F.relu(self.fc1(x))\n","        mu = self.fc_mu(h)\n","        logvar = self.fc_logvar(h)\n","        return mu, logvar\n","\n","    def reparameterize(self, mu, logvar):\n","        \"\"\"Sample z from N(mu, sigma^2) via reparameterization trick.\"\"\"\n","        std = torch.exp(0.5 * logvar)        # standard deviation = exp(0.5*logvar)\n","        eps = torch.randn_like(std)          # draw random epsilon ~ N(0, I)\n","        z = mu + eps * std                   # shift and scale by mu and sigma\n","        return z\n","\n","    def decode(self, z):\n","        \"\"\"Decode latent vector z to reconstruct an image.\"\"\"\n","        h = F.relu(self.fc_dec1(z))\n","        x_reconst = torch.sigmoid(self.fc_dec2(h))  # sigmoid to get pixel intensities 0-1\n","        return x_reconst\n","\n","    def forward(self, x):\n","        \"\"\"Perform encoding, reparameterization, and decoding.\"\"\"\n","        mu, logvar = self.encode(x.view(-1, 28*28))  # flatten image to vector\n","        z = self.reparameterize(mu, logvar)\n","        x_reconst = self.decode(z)\n","        return x_reconst, mu, logvar"]},{"cell_type":"markdown","id":"b3c5abae","metadata":{"id":"b3c5abae"},"source":["## âš™ï¸ Step 4: Initialize Model and Optimizer"]},{"cell_type":"code","execution_count":null,"id":"3f6e6829","metadata":{"id":"3f6e6829"},"outputs":[],"source":["# Initialize model and optimizer\n","model = VAE().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# VAE loss: binary cross-entropy (reconstruction) + Kullback-Leibler divergence\n","def vae_loss(recon_x, x, mu, logvar):\n","    # Reconstruction loss (BCE) summed over all pixels\n","    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 28*28), reduction='sum')\n","    # KL Divergence loss: forces q(z|x) to approach p(z) = N(0,I)\n","    # Formula: 0.5 * sum( exp(logÏƒ^2) + Î¼^2 - 1 - logÏƒ^2 )\n","    KLD = 0.5 * torch.sum(torch.exp(logvar) + mu**2 - 1 - logvar)\n","    return BCE + KLD"]},{"cell_type":"markdown","id":"9d0ad98e","metadata":{"id":"9d0ad98e"},"source":["## ðŸ’¾ Step 6: Setup Output Directory"]},{"cell_type":"code","execution_count":null,"id":"5c4729fa","metadata":{"id":"5c4729fa"},"outputs":[],"source":["# Directory to save generated images\n","os.makedirs('vae_outputs', exist_ok=True)"]},{"cell_type":"markdown","id":"259bd410","metadata":{"id":"259bd410"},"source":["## ðŸ” Step 7: Training and Validation Functions"]},{"cell_type":"code","execution_count":null,"id":"5f51a2cc","metadata":{"id":"5f51a2cc"},"outputs":[],"source":["# Training and Validation loops\n","def train(epoch):\n","    model.train()\n","    train_loss = 0\n","    for batch_idx, (data, _) in enumerate(train_loader):\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        recon_batch, mu, logvar = model(data)                   # forward pass through VAE\n","        loss = vae_loss(recon_batch, data, mu, logvar)          # compute VAE loss\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","        if batch_idx % 100 == 0:\n","            print(f\"Train Epoch {epoch} [{batch_idx*len(data)}/{len(train_loader.dataset)}] Loss: {loss.item()/len(data):.4f}\")\n","    avg_loss = train_loss / len(train_loader.dataset)\n","    print(f\"===> Epoch {epoch} Complete: Avg Train Loss: {avg_loss:.4f}\")\n","\n","def validate(epoch):\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch_idx, (data, _) in enumerate(test_loader):\n","            data = data.to(device)\n","            recon_batch, mu, logvar = model(data)\n","            loss = vae_loss(recon_batch, data, mu, logvar)\n","            val_loss += loss.item()\n","            # Save input vs reconstruction comparison for the first batch\n","            if batch_idx == 0:\n","                n = min(data.size(0), 8)\n","                comparison = torch.cat([data[:n], recon_batch.view(-1,1,28,28)[:n]])\n","                utils.save_image(comparison.cpu(), f\"vae_outputs/reconstruction_epoch{epoch}.png\", nrow=n)\n","        avg_loss = val_loss / len(test_loader.dataset)\n","        print(f\"===> Validation: Avg Loss: {avg_loss:.4f}\")"]},{"cell_type":"markdown","id":"69d5aec2","metadata":{"id":"69d5aec2"},"source":["## ðŸš€ Step 8: Train the VAE and Generate Samples"]},{"cell_type":"code","execution_count":null,"id":"de9b9c19","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"de9b9c19","executionInfo":{"status":"ok","timestamp":1744298917312,"user_tz":-120,"elapsed":445568,"user":{"displayName":"Piergiuseppe Liuzzi","userId":"11508690843381073106"}},"outputId":"0621323d-0f2a-406b-afe3-c70e19c8f41c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch 1 [0/60000] Loss: 553.1377\n","Train Epoch 1 [12800/60000] Loss: 281.6621\n","Train Epoch 1 [25600/60000] Loss: 299.7235\n","Train Epoch 1 [38400/60000] Loss: 275.5094\n","Train Epoch 1 [51200/60000] Loss: 278.4427\n","===> Epoch 1 Complete: Avg Train Loss: 291.2910\n","===> Validation: Avg Loss: 275.7749\n","Train Epoch 2 [0/60000] Loss: 269.3788\n","Train Epoch 2 [12800/60000] Loss: 270.6135\n","Train Epoch 2 [25600/60000] Loss: 261.4965\n","Train Epoch 2 [38400/60000] Loss: 285.4909\n","Train Epoch 2 [51200/60000] Loss: 269.4290\n","===> Epoch 2 Complete: Avg Train Loss: 272.2193\n","===> Validation: Avg Loss: 271.5556\n","Train Epoch 3 [0/60000] Loss: 269.3295\n","Train Epoch 3 [12800/60000] Loss: 280.1384\n","Train Epoch 3 [25600/60000] Loss: 278.4810\n","Train Epoch 3 [38400/60000] Loss: 256.9223\n","Train Epoch 3 [51200/60000] Loss: 266.5945\n","===> Epoch 3 Complete: Avg Train Loss: 268.9934\n","===> Validation: Avg Loss: 268.7676\n","Train Epoch 4 [0/60000] Loss: 268.6575\n","Train Epoch 4 [12800/60000] Loss: 285.8950\n","Train Epoch 4 [25600/60000] Loss: 264.5288\n","Train Epoch 4 [38400/60000] Loss: 270.3225\n","Train Epoch 4 [51200/60000] Loss: 248.5983\n","===> Epoch 4 Complete: Avg Train Loss: 267.1097\n","===> Validation: Avg Loss: 267.9840\n","Train Epoch 5 [0/60000] Loss: 273.7837\n","Train Epoch 5 [12800/60000] Loss: 260.8546\n","Train Epoch 5 [25600/60000] Loss: 259.3487\n","Train Epoch 5 [38400/60000] Loss: 262.1754\n","Train Epoch 5 [51200/60000] Loss: 264.5179\n","===> Epoch 5 Complete: Avg Train Loss: 265.9384\n","===> Validation: Avg Loss: 266.5551\n","[Saved 64 sampled images at epoch 5]\n","Train Epoch 6 [0/60000] Loss: 264.0268\n","Train Epoch 6 [12800/60000] Loss: 258.9240\n","Train Epoch 6 [25600/60000] Loss: 263.8750\n","Train Epoch 6 [38400/60000] Loss: 254.2872\n","Train Epoch 6 [51200/60000] Loss: 279.6799\n","===> Epoch 6 Complete: Avg Train Loss: 264.9848\n","===> Validation: Avg Loss: 266.3143\n","Train Epoch 7 [0/60000] Loss: 267.9763\n","Train Epoch 7 [12800/60000] Loss: 271.5656\n","Train Epoch 7 [25600/60000] Loss: 254.1796\n","Train Epoch 7 [38400/60000] Loss: 262.0234\n","Train Epoch 7 [51200/60000] Loss: 261.6739\n","===> Epoch 7 Complete: Avg Train Loss: 264.3054\n","===> Validation: Avg Loss: 265.4550\n","Train Epoch 8 [0/60000] Loss: 263.4477\n","Train Epoch 8 [12800/60000] Loss: 253.6137\n","Train Epoch 8 [25600/60000] Loss: 257.9743\n","Train Epoch 8 [38400/60000] Loss: 261.6599\n","Train Epoch 8 [51200/60000] Loss: 255.0870\n","===> Epoch 8 Complete: Avg Train Loss: 263.6791\n","===> Validation: Avg Loss: 264.9312\n","Train Epoch 9 [0/60000] Loss: 247.3313\n","Train Epoch 9 [12800/60000] Loss: 253.5758\n","Train Epoch 9 [25600/60000] Loss: 263.8832\n","Train Epoch 9 [38400/60000] Loss: 255.3115\n","Train Epoch 9 [51200/60000] Loss: 264.4610\n","===> Epoch 9 Complete: Avg Train Loss: 263.2569\n","===> Validation: Avg Loss: 264.7492\n","Train Epoch 10 [0/60000] Loss: 263.7570\n","Train Epoch 10 [12800/60000] Loss: 255.3246\n","Train Epoch 10 [25600/60000] Loss: 270.3383\n","Train Epoch 10 [38400/60000] Loss: 264.7116\n","Train Epoch 10 [51200/60000] Loss: 252.8149\n","===> Epoch 10 Complete: Avg Train Loss: 262.8527\n","===> Validation: Avg Loss: 264.3309\n","[Saved 64 sampled images at epoch 10]\n","Train Epoch 11 [0/60000] Loss: 261.2378\n","Train Epoch 11 [12800/60000] Loss: 259.1764\n","Train Epoch 11 [25600/60000] Loss: 245.3149\n","Train Epoch 11 [38400/60000] Loss: 254.6815\n","Train Epoch 11 [51200/60000] Loss: 259.5057\n","===> Epoch 11 Complete: Avg Train Loss: 262.5278\n","===> Validation: Avg Loss: 264.0468\n","Train Epoch 12 [0/60000] Loss: 251.3298\n","Train Epoch 12 [12800/60000] Loss: 266.7112\n","Train Epoch 12 [25600/60000] Loss: 273.3591\n","Train Epoch 12 [38400/60000] Loss: 244.6428\n","Train Epoch 12 [51200/60000] Loss: 251.8161\n","===> Epoch 12 Complete: Avg Train Loss: 262.1421\n","===> Validation: Avg Loss: 263.4845\n","Train Epoch 13 [0/60000] Loss: 252.9220\n","Train Epoch 13 [12800/60000] Loss: 258.7945\n","Train Epoch 13 [25600/60000] Loss: 261.3331\n","Train Epoch 13 [38400/60000] Loss: 261.6974\n","Train Epoch 13 [51200/60000] Loss: 254.7410\n","===> Epoch 13 Complete: Avg Train Loss: 261.8794\n","===> Validation: Avg Loss: 263.5149\n","Train Epoch 14 [0/60000] Loss: 241.1711\n","Train Epoch 14 [12800/60000] Loss: 257.7256\n","Train Epoch 14 [25600/60000] Loss: 259.5938\n","Train Epoch 14 [38400/60000] Loss: 255.7852\n","Train Epoch 14 [51200/60000] Loss: 266.1544\n","===> Epoch 14 Complete: Avg Train Loss: 261.6435\n","===> Validation: Avg Loss: 262.9265\n","Train Epoch 15 [0/60000] Loss: 254.1498\n","Train Epoch 15 [12800/60000] Loss: 265.8051\n","Train Epoch 15 [25600/60000] Loss: 276.6891\n","Train Epoch 15 [38400/60000] Loss: 260.2934\n","Train Epoch 15 [51200/60000] Loss: 262.6396\n","===> Epoch 15 Complete: Avg Train Loss: 261.4174\n","===> Validation: Avg Loss: 262.8954\n","[Saved 64 sampled images at epoch 15]\n","Train Epoch 16 [0/60000] Loss: 263.0924\n","Train Epoch 16 [12800/60000] Loss: 257.5959\n","Train Epoch 16 [25600/60000] Loss: 261.7092\n","Train Epoch 16 [38400/60000] Loss: 271.8501\n","Train Epoch 16 [51200/60000] Loss: 259.7962\n","===> Epoch 16 Complete: Avg Train Loss: 261.1528\n","===> Validation: Avg Loss: 262.9680\n","Train Epoch 17 [0/60000] Loss: 264.9379\n","Train Epoch 17 [12800/60000] Loss: 252.7007\n","Train Epoch 17 [25600/60000] Loss: 272.2415\n","Train Epoch 17 [38400/60000] Loss: 260.5661\n","Train Epoch 17 [51200/60000] Loss: 265.1552\n","===> Epoch 17 Complete: Avg Train Loss: 261.0393\n","===> Validation: Avg Loss: 262.7290\n","Train Epoch 18 [0/60000] Loss: 271.1805\n","Train Epoch 18 [12800/60000] Loss: 254.4875\n","Train Epoch 18 [25600/60000] Loss: 241.1605\n","Train Epoch 18 [38400/60000] Loss: 272.3817\n","Train Epoch 18 [51200/60000] Loss: 266.0048\n","===> Epoch 18 Complete: Avg Train Loss: 260.7755\n","===> Validation: Avg Loss: 262.8121\n","Train Epoch 19 [0/60000] Loss: 263.4540\n","Train Epoch 19 [12800/60000] Loss: 261.6308\n","Train Epoch 19 [25600/60000] Loss: 262.4643\n","Train Epoch 19 [38400/60000] Loss: 258.1154\n","Train Epoch 19 [51200/60000] Loss: 266.1579\n","===> Epoch 19 Complete: Avg Train Loss: 260.5921\n","===> Validation: Avg Loss: 262.1210\n","Train Epoch 20 [0/60000] Loss: 245.6525\n","Train Epoch 20 [12800/60000] Loss: 262.9240\n","Train Epoch 20 [25600/60000] Loss: 249.9936\n","Train Epoch 20 [38400/60000] Loss: 251.9289\n","Train Epoch 20 [51200/60000] Loss: 259.7607\n","===> Epoch 20 Complete: Avg Train Loss: 260.4936\n","===> Validation: Avg Loss: 262.1669\n","[Saved 64 sampled images at epoch 20]\n","VAE training complete. Outputs saved to 'vae_outputs/' directory.\n"]}],"source":["# Training loop with image generation\n","train_loss, val_loss = [], []\n","for epoch in range(1, epochs+1):\n","    train(epoch)\n","    validate(epoch)\n","    # Every few epochs, sample new images from the latent space prior\n","    if epoch % 5 == 0:\n","        model.eval()\n","        with torch.no_grad():\n","            # Sample 64 random latent vectors from N(0, I)\n","            z = torch.randn(64, latent_dim).to(device)\n","            sample_imgs = model.decode(z).view(-1, 1, 28, 28)\n","            utils.save_image(sample_imgs.cpu(), f\"vae_outputs/sample_epoch{epoch}.png\", nrow=8)\n","            print(f\"[Saved 64 sampled images at epoch {epoch}]\")\n","\n","# After training, if latent_dim == 2, visualize the latent space structure\n","if latent_dim == 2:\n","    model.eval()\n","    zs = []\n","    labels = []\n","    with torch.no_grad():\n","        for data, label in test_loader:\n","            data = data.to(device)\n","            mu, logvar = model.encode(data.view(-1, 28*28))\n","            zs.append(mu.cpu())   # use mean as representative latent point\n","            labels += label.tolist()\n","    zs = torch.cat(zs, dim=0).numpy()\n","    labels = torch.tensor(labels).numpy()\n","    # Scatter plot of latent encodings colored by true class label\n","    plt.figure(figsize=(6,6))\n","    plt.scatter(zs[:,0], zs[:,1], c=labels, cmap='tab10', s=5, alpha=0.7)\n","    plt.colorbar().set_label(\"Fashion-MNIST class\")\n","    plt.title(\"VAE latent space (2D) for test images\")\n","    plt.xlabel(\"z1\"); plt.ylabel(\"z2\")\n","    plt.savefig(\"vae_outputs/latent_space_scatter.png\")\n","    plt.close()\n","    # Also create a grid of images by traversing the 2D latent space\n","    grid_x = torch.linspace(-3, 3, steps=20)\n","    grid_y = torch.linspace(-3, 3, steps=20)\n","    imgs = []\n","    with torch.no_grad():\n","        for yi, yv in enumerate(grid_y):\n","            for xi, xv in enumerate(grid_x):\n","                z = torch.tensor([[xv.item(), yv.item()]], device=device)\n","                x_hat = model.decode(z).view(1,1,28,28)\n","                imgs.append(x_hat)\n","    imgs = torch.cat(imgs, dim=0)\n","    # Save 20x20 grid of generated images covering the latent space\n","    utils.save_image(imgs, \"vae_outputs/latent_space_grid.png\", nrow=20)\n","print(\"VAE training complete. Outputs saved to 'vae_outputs/' directory.\")"]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}