{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch K-Fold CV for Regression"
      ],
      "metadata": {
        "id": "85YiUHt4W2jP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBx3Qe1pWadN",
        "outputId": "1d540d82-8853-4a7c-c005-88ad9267bb99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating synthetic data...\n",
            "Data generated: 500 samples, 10 features.\n",
            "\n",
            "Starting 5-Fold Cross-Validation...\n",
            "\n",
            "--- Fold 1/5 ---\n",
            "Train samples: 400, Validation samples: 100\n",
            "Training...\n",
            "  Epoch [10/50], Train Loss: 8302.0220\n",
            "  Epoch [20/50], Train Loss: 391.8277\n",
            "  Epoch [30/50], Train Loss: 310.9277\n",
            "  Epoch [40/50], Train Loss: 274.5235\n",
            "  Epoch [50/50], Train Loss: 249.5684\n",
            "Validating...\n",
            "--- Fold 1 Validation MSE: 317.9261 ---\n",
            "\n",
            "--- Fold 2/5 ---\n",
            "Train samples: 400, Validation samples: 100\n",
            "Training...\n",
            "  Epoch [10/50], Train Loss: 7245.0312\n",
            "  Epoch [20/50], Train Loss: 361.5749\n",
            "  Epoch [30/50], Train Loss: 280.0867\n",
            "  Epoch [40/50], Train Loss: 251.2109\n",
            "  Epoch [50/50], Train Loss: 226.2608\n",
            "Validating...\n",
            "--- Fold 2 Validation MSE: 279.0495 ---\n",
            "\n",
            "--- Fold 3/5 ---\n",
            "Train samples: 400, Validation samples: 100\n",
            "Training...\n",
            "  Epoch [10/50], Train Loss: 7414.5510\n",
            "  Epoch [20/50], Train Loss: 354.0877\n",
            "  Epoch [30/50], Train Loss: 293.9479\n",
            "  Epoch [40/50], Train Loss: 261.6438\n",
            "  Epoch [50/50], Train Loss: 235.8877\n",
            "Validating...\n",
            "--- Fold 3 Validation MSE: 314.3125 ---\n",
            "\n",
            "--- Fold 4/5 ---\n",
            "Train samples: 400, Validation samples: 100\n",
            "Training...\n",
            "  Epoch [10/50], Train Loss: 8310.9828\n",
            "  Epoch [20/50], Train Loss: 366.8772\n",
            "  Epoch [30/50], Train Loss: 289.3982\n",
            "  Epoch [40/50], Train Loss: 256.3477\n",
            "  Epoch [50/50], Train Loss: 229.4607\n",
            "Validating...\n",
            "--- Fold 4 Validation MSE: 325.8239 ---\n",
            "\n",
            "--- Fold 5/5 ---\n",
            "Train samples: 400, Validation samples: 100\n",
            "Training...\n",
            "  Epoch [10/50], Train Loss: 8660.4236\n",
            "  Epoch [20/50], Train Loss: 402.1604\n",
            "  Epoch [30/50], Train Loss: 311.1535\n",
            "  Epoch [40/50], Train Loss: 274.4479\n",
            "  Epoch [50/50], Train Loss: 247.5183\n",
            "Validating...\n",
            "--- Fold 5 Validation MSE: 329.3204 ---\n",
            "\n",
            "--- Cross-Validation Results ---\n",
            "Validation MSE for each fold: ['317.9261', '279.0495', '314.3125', '325.8239', '329.3204']\n",
            "Average Validation MSE across 5 folds: 313.2865\n",
            "Standard Deviation of Validation MSE: 17.9389\n",
            "\n",
            "K-Fold Cross-Validation finished.\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "PyTorch K-Fold Cross-Validation Example for Regression\n",
        "\n",
        "This script demonstrates how to implement K-Fold Cross-Validation\n",
        "with PyTorch for a simple regression problem.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_regression # Using make_regression for simplicity\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "N_SPLITS = 5        # Number of folds for K-Fold CV\n",
        "N_EPOCHS = 50       # Number of training epochs per fold\n",
        "BATCH_SIZE = 16     # Batch size for training\n",
        "LEARNING_RATE = 0.001 # Learning rate for the optimizer\n",
        "N_SAMPLES = 500     # Total number of samples in the dataset\n",
        "N_FEATURES = 10     # Number of features for the synthetic data\n",
        "RANDOM_SEED = 42    # Seed for reproducibility\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "# --- 2. Generate Synthetic Regression Data ---\n",
        "print(\"Generating synthetic data...\")\n",
        "# Using sklearn's make_regression for a controlled example\n",
        "X, y = make_regression(n_samples=N_SAMPLES, n_features=N_FEATURES, noise=15.0, random_state=RANDOM_SEED)\n",
        "\n",
        "# Reshape y to be a 2D tensor [n_samples, 1] as expected by MSELoss\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "# --- 3. Data Preprocessing ---\n",
        "# It's crucial to scale features, especially for neural networks\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X) # Fit and transform on the whole dataset for simplicity here\n",
        "                                   # In a real scenario, fit ONLY on training data within the fold\n",
        "\n",
        "# Convert numpy arrays to PyTorch tensors\n",
        "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# Create a full dataset\n",
        "full_dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "print(f\"Data generated: {X_tensor.shape[0]} samples, {X_tensor.shape[1]} features.\")\n",
        "\n",
        "# --- 4. Define the Neural Network Model ---\n",
        "class RegressionNet(nn.Module):\n",
        "    \"\"\"A simple feed-forward neural network for regression.\"\"\"\n",
        "    def __init__(self, input_features):\n",
        "        super(RegressionNet, self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_features, 64)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.layer_2 = nn.Linear(64, 32)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.output_layer = nn.Linear(32, 1) # Output layer has 1 neuron for regression\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "# --- 5. K-Fold Cross-Validation Setup ---\n",
        "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
        "fold_results = [] # To store validation loss for each fold\n",
        "\n",
        "print(f\"\\nStarting {N_SPLITS}-Fold Cross-Validation...\")\n",
        "\n",
        "# --- 6. K-Fold Cross-Validation Loop ---\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(full_dataset)):\n",
        "    print(f\"\\n--- Fold {fold+1}/{N_SPLITS} ---\")\n",
        "\n",
        "    # --- Data Splitting for the Current Fold ---\n",
        "    # Create subsets for training and validation based on indices from KFold\n",
        "    train_subset = torch.utils.data.Subset(full_dataset, train_idx)\n",
        "    val_subset = torch.utils.data.Subset(full_dataset, val_idx)\n",
        "\n",
        "    # Create DataLoaders for the current fold\n",
        "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False) # No need to shuffle validation data\n",
        "\n",
        "    print(f\"Train samples: {len(train_subset)}, Validation samples: {len(val_subset)}\")\n",
        "\n",
        "    # --- Model, Loss, Optimizer Initialization (re-initialize for each fold) ---\n",
        "    model = RegressionNet(input_features=N_FEATURES)\n",
        "    criterion = nn.MSELoss() # Mean Squared Error loss for regression\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # --- Training Loop for the Current Fold ---\n",
        "    print(\"Training...\")\n",
        "    for epoch in range(N_EPOCHS):\n",
        "        model.train() # Set model to training mode\n",
        "        epoch_train_loss = 0.0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_train_loss += loss.item() * batch_X.size(0) # Accumulate loss weighted by batch size\n",
        "\n",
        "        avg_epoch_train_loss = epoch_train_loss / len(train_subset)\n",
        "        # Optional: Print training progress less frequently\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "             print(f'  Epoch [{epoch+1}/{N_EPOCHS}], Train Loss: {avg_epoch_train_loss:.4f}')\n",
        "\n",
        "    # --- Validation Loop for the Current Fold ---\n",
        "    print(\"Validating...\")\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    fold_val_loss = 0.0\n",
        "    with torch.no_grad(): # Disable gradient calculation during validation\n",
        "        for batch_X, batch_y in val_loader:\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            fold_val_loss += loss.item() * batch_X.size(0) # Accumulate loss\n",
        "\n",
        "    avg_fold_val_loss = fold_val_loss / len(val_subset)\n",
        "    fold_results.append(avg_fold_val_loss)\n",
        "    print(f\"--- Fold {fold+1} Validation MSE: {avg_fold_val_loss:.4f} ---\")\n",
        "\n",
        "# --- 7. Results ---\n",
        "average_val_loss = np.mean(fold_results)\n",
        "std_dev_val_loss = np.std(fold_results)\n",
        "\n",
        "print(\"\\n--- Cross-Validation Results ---\")\n",
        "print(f\"Validation MSE for each fold: {[f'{loss:.4f}' for loss in fold_results]}\")\n",
        "print(f\"Average Validation MSE across {N_SPLITS} folds: {average_val_loss:.4f}\")\n",
        "print(f\"Standard Deviation of Validation MSE: {std_dev_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nK-Fold Cross-Validation finished.\")\n",
        "\n",
        "# --- Optional: Train Final Model on Full Data (if needed) ---\n",
        "# After finding good hyperparameters using CV, you might train one final\n",
        "# model on the entire dataset for deployment.\n",
        "# print(\"\\nTraining final model on full dataset (optional)...\")\n",
        "# final_model = RegressionNet(input_features=N_FEATURES)\n",
        "# final_criterion = nn.MSELoss()\n",
        "# final_optimizer = optim.Adam(final_model.parameters(), lr=LEARNING_RATE)\n",
        "# full_loader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "#\n",
        "# for epoch in range(N_EPOCHS): # Use the same number of epochs or adjust\n",
        "#     final_model.train()\n",
        "#     for batch_X, batch_y in full_loader:\n",
        "#         final_optimizer.zero_grad()\n",
        "#         outputs = final_model(batch_X)\n",
        "#         loss = final_criterion(outputs, batch_y)\n",
        "#         loss.backward()\n",
        "#         final_optimizer.step()\n",
        "#     if (epoch + 1) % 10 == 0:\n",
        "#          print(f'  Final Model - Epoch [{epoch+1}/{N_EPOCHS}], Loss: {loss.item():.4f}') # Loss of last batch\n",
        "# print(\"Final model training complete.\")\n",
        "# # You can now save and use 'final_model'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "PyTorch K-Fold Cross-Validation with Hyperparameter Tuning for Regression\n",
        "\n",
        "This script demonstrates how to implement K-Fold Cross-Validation\n",
        "with PyTorch for a simple regression problem and includes a basic\n",
        "grid search for hyperparameter tuning (learning rate and hidden layer size).\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_regression\n",
        "import itertools # To iterate over hyperparameter combinations\n",
        "import time\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "N_SPLITS = 5        # Number of folds for K-Fold CV\n",
        "N_EPOCHS = 30       # Number of training epochs per fold (reduced for faster demo)\n",
        "BATCH_SIZE = 32     # Batch size for training\n",
        "N_SAMPLES = 500     # Total number of samples in the dataset\n",
        "N_FEATURES = 10     # Number of features for the synthetic data\n",
        "RANDOM_SEED = 42    # Seed for reproducibility\n",
        "\n",
        "# --- Hyperparameter Search Space ---\n",
        "# Define the hyperparameters you want to tune\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.001], # Example learning rates\n",
        "    'hidden_size_1': [32, 64]       # Example sizes for the first hidden layer\n",
        "    # Add more hyperparameters here if needed (e.g., 'hidden_size_2', 'batch_size')\n",
        "}\n",
        "\n",
        "# Generate all combinations of hyperparameters\n",
        "hyperparameter_combinations = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
        "\n",
        "print(f\"Starting Hyperparameter Search with {len(hyperparameter_combinations)} combinations.\")\n",
        "print(\"Hyperparameter combinations to test:\")\n",
        "for combo in hyperparameter_combinations:\n",
        "    print(f\"  - {combo}\")\n",
        "\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# --- 2. Generate Synthetic Regression Data ---\n",
        "print(\"\\nGenerating synthetic data...\")\n",
        "X, y = make_regression(n_samples=N_SAMPLES, n_features=N_FEATURES, noise=20.0, random_state=RANDOM_SEED)\n",
        "y = y.reshape(-1, 1) # Reshape y for MSELoss\n",
        "\n",
        "# --- 3. Data Preprocessing (Outside the loop for this example) ---\n",
        "# NOTE: Ideally, scaling should be done *inside* the K-Fold loop\n",
        "#       fitting the scaler ONLY on the training data of that fold.\n",
        "#       Doing it outside is simpler for demonstration but introduces slight data leakage.\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Convert numpy arrays to PyTorch tensors\n",
        "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "full_dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "print(f\"Data generated: {X_tensor.shape[0]} samples, {X_tensor.shape[1]} features.\")\n",
        "\n",
        "# --- 4. Define the Neural Network Model ---\n",
        "class RegressionNet(nn.Module):\n",
        "    \"\"\"A simple feed-forward neural network for regression.\"\"\"\n",
        "    def __init__(self, input_features, hidden_size_1=64): # Default value\n",
        "        super(RegressionNet, self).__init__()\n",
        "        # Use the passed hidden_size_1 for flexibility\n",
        "        self.layer_1 = nn.Linear(input_features, hidden_size_1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        # Adjust subsequent layers if needed, here we keep it simple\n",
        "        self.layer_2 = nn.Linear(hidden_size_1, hidden_size_1 // 2) # Example: second layer depends on first\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.output_layer = nn.Linear(hidden_size_1 // 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "# --- 5. Hyperparameter Tuning Loop ---\n",
        "best_hyperparams = None\n",
        "best_avg_val_loss = float('inf') # Initialize with infinity\n",
        "results_log = [] # Store results for each hyperparameter set\n",
        "\n",
        "start_time_tuning = time.time()\n",
        "\n",
        "for params in hyperparameter_combinations:\n",
        "    current_lr = params['learning_rate']\n",
        "    current_hidden_size = params['hidden_size_1']\n",
        "    print(f\"\\n--- Testing Hyperparameters: LR={current_lr}, HiddenSize1={current_hidden_size} ---\")\n",
        "\n",
        "    # --- 6. K-Fold Cross-Validation Setup (Inside Hyperparameter Loop) ---\n",
        "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
        "    fold_val_losses = [] # Store validation loss for each fold *for this set of hyperparameters*\n",
        "    fold_start_time = time.time()\n",
        "\n",
        "    # --- 7. K-Fold Cross-Validation Loop ---\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(full_dataset)):\n",
        "        # print(f\"  Fold {fold+1}/{N_SPLITS}...\") # Less verbose output\n",
        "\n",
        "        # --- Data Splitting for the Current Fold ---\n",
        "        train_subset = Subset(full_dataset, train_idx)\n",
        "        val_subset = Subset(full_dataset, val_idx)\n",
        "        train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        # --- Model, Loss, Optimizer Initialization (Using current hyperparameters) ---\n",
        "        # Pass the current hidden size to the model\n",
        "        model = RegressionNet(input_features=N_FEATURES, hidden_size_1=current_hidden_size)\n",
        "        criterion = nn.MSELoss()\n",
        "        # Use the current learning rate for the optimizer\n",
        "        optimizer = optim.Adam(model.parameters(), lr=current_lr)\n",
        "\n",
        "        # --- Training Loop for the Current Fold ---\n",
        "        for epoch in range(N_EPOCHS):\n",
        "            model.train()\n",
        "            for batch_X, batch_y in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            # No epoch printing during hyperparameter search to reduce clutter\n",
        "\n",
        "        # --- Validation Loop for the Current Fold ---\n",
        "        model.eval()\n",
        "        fold_val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in val_loader:\n",
        "                outputs = model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                fold_val_loss += loss.item() * batch_X.size(0)\n",
        "\n",
        "        avg_fold_val_loss = fold_val_loss / len(val_subset)\n",
        "        fold_val_losses.append(avg_fold_val_loss)\n",
        "        # print(f\"    Fold {fold+1} Val MSE: {avg_fold_val_loss:.4f}\") # Less verbose\n",
        "\n",
        "    # --- Calculate Average Performance for this Hyperparameter Set ---\n",
        "    average_loss_for_params = np.mean(fold_val_losses)\n",
        "    std_dev_for_params = np.std(fold_val_losses)\n",
        "    fold_end_time = time.time()\n",
        "\n",
        "    print(f\"  Average Validation MSE for {params}: {average_loss_for_params:.4f} +/- {std_dev_for_params:.4f}\")\n",
        "    print(f\"  Time taken for this set: {fold_end_time - fold_start_time:.2f} seconds\")\n",
        "\n",
        "    results_log.append({\n",
        "        'params': params,\n",
        "        'avg_val_loss': average_loss_for_params,\n",
        "        'std_dev_val_loss': std_dev_for_params\n",
        "    })\n",
        "\n",
        "    # --- Update Best Hyperparameters ---\n",
        "    if average_loss_for_params < best_avg_val_loss:\n",
        "        best_avg_val_loss = average_loss_for_params\n",
        "        best_hyperparams = params\n",
        "        print(f\"  ** New best hyperparameters found! **\")\n",
        "\n",
        "end_time_tuning = time.time()\n",
        "print(f\"\\nTotal Hyperparameter Tuning Time: {end_time_tuning - start_time_tuning:.2f} seconds\")\n",
        "\n",
        "# --- 8. Final Results ---\n",
        "print(\"\\n--- Hyperparameter Tuning Results ---\")\n",
        "if best_hyperparams:\n",
        "    print(f\"Best Hyperparameters Found:\")\n",
        "    for key, value in best_hyperparams.items():\n",
        "        print(f\"  - {key}: {value}\")\n",
        "    print(f\"Best Average Validation MSE: {best_avg_val_loss:.4f}\")\n",
        "else:\n",
        "    print(\"Hyperparameter tuning did not complete successfully or no parameters were tested.\")\n",
        "\n",
        "# --- Optional: Train Final Model with Best Hyperparameters ---\n",
        "# if best_hyperparams:\n",
        "#     print(\"\\nTraining final model on full dataset with best hyperparameters...\")\n",
        "#     final_model = RegressionNet(input_features=N_FEATURES, hidden_size_1=best_hyperparams['hidden_size_1'])\n",
        "#     final_criterion = nn.MSELoss()\n",
        "#     final_optimizer = optim.Adam(final_model.parameters(), lr=best_hyperparams['learning_rate'])\n",
        "#     full_loader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "#\n",
        "#     for epoch in range(N_EPOCHS): # Use the same number of epochs or adjust\n",
        "#         final_model.train()\n",
        "#         epoch_loss = 0.0\n",
        "#         for batch_X, batch_y in full_loader:\n",
        "#             final_optimizer.zero_grad()\n",
        "#             outputs = final_model(batch_X)\n",
        "#             loss = final_criterion(outputs, batch_y)\n",
        "#             loss.backward()\n",
        "#             final_optimizer.step()\n",
        "#             epoch_loss += loss.item() * batch_X.size(0)\n",
        "#\n",
        "#         avg_epoch_loss = epoch_loss / len(full_dataset)\n",
        "#         if (epoch + 1) % 10 == 0:\n",
        "#              print(f'  Final Model - Epoch [{epoch+1}/{N_EPOCHS}], Train Loss: {avg_epoch_loss:.4f}')\n",
        "#     print(\"Final model training complete.\")\n",
        "#     # You can now save and use 'final_model'\n",
        "\n",
        "print(\"\\nScript finished.\")\n"
      ],
      "metadata": {
        "id": "JL-J5TA-ZeME",
        "outputId": "bd49b839-3783-4175-8ca3-b272e45fb864",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Hyperparameter Search with 4 combinations.\n",
            "Hyperparameter combinations to test:\n",
            "  - {'learning_rate': 0.01, 'hidden_size_1': 32}\n",
            "  - {'learning_rate': 0.01, 'hidden_size_1': 64}\n",
            "  - {'learning_rate': 0.001, 'hidden_size_1': 32}\n",
            "  - {'learning_rate': 0.001, 'hidden_size_1': 64}\n",
            "\n",
            "Generating synthetic data...\n",
            "Data generated: 500 samples, 10 features.\n",
            "\n",
            "--- Testing Hyperparameters: LR=0.01, HiddenSize1=32 ---\n",
            "  Average Validation MSE for {'learning_rate': 0.01, 'hidden_size_1': 32}: 436.6098 +/- 43.8240\n",
            "  Time taken for this set: 3.35 seconds\n",
            "  ** New best hyperparameters found! **\n",
            "\n",
            "--- Testing Hyperparameters: LR=0.01, HiddenSize1=64 ---\n",
            "  Average Validation MSE for {'learning_rate': 0.01, 'hidden_size_1': 64}: 445.4537 +/- 33.8663\n",
            "  Time taken for this set: 3.96 seconds\n",
            "\n",
            "--- Testing Hyperparameters: LR=0.001, HiddenSize1=32 ---\n",
            "  Average Validation MSE for {'learning_rate': 0.001, 'hidden_size_1': 32}: 4661.9173 +/- 642.2490\n",
            "  Time taken for this set: 3.27 seconds\n",
            "\n",
            "--- Testing Hyperparameters: LR=0.001, HiddenSize1=64 ---\n",
            "  Average Validation MSE for {'learning_rate': 0.001, 'hidden_size_1': 64}: 807.9063 +/- 69.9696\n",
            "  Time taken for this set: 3.05 seconds\n",
            "\n",
            "Total Hyperparameter Tuning Time: 13.63 seconds\n",
            "\n",
            "--- Hyperparameter Tuning Results ---\n",
            "Best Hyperparameters Found:\n",
            "  - learning_rate: 0.01\n",
            "  - hidden_size_1: 32\n",
            "Best Average Validation MSE: 436.6098\n",
            "\n",
            "Script finished.\n"
          ]
        }
      ]
    }
  ]
}