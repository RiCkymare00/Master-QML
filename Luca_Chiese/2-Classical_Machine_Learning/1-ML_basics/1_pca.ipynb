{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # this module contains functions that allow to quickly generate many kinds of plots .\n",
    "import mpl_toolkits.mplot3d # provides 3D plotting\n",
    "from sklearn import datasets # this module includes utilities to load datasets\n",
    "from sklearn.decomposition import PCA #principal component analysis\n",
    "import numpy as np #library that contains multidimensional array and matrix data structures\n",
    "import cv2 #real-time optimized Computer Vision library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The Iris data set consists of 3 different types of irises’ (Setosa, Versicolour,and Virginica) petal and sepal length, stored in a 150x4 (n_samples x n_features) numpy.ndarray.\n",
    "\n",
    "The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width\n",
    "\"\"\"\n",
    "\n",
    "# import some data to play with\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "'''\n",
    "Load and return the iris dataset (classification).\n",
    "\n",
    "The iris dataset is a classic and very easy multi-class classification dataset.\n",
    "\n",
    "Classes: 3\n",
    "Samples per class: 50\n",
    "Samples total: 150\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, :2]  # the data is 4-dimensional (4 features) but we only take the first two features for graphic purposes (we can visualize 2 features at a time using a scatter plot).\n",
    "y = iris.target\n",
    "\n",
    "print(\"feature matrix size\", X.shape)\n",
    "print(\"label vector size\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the features (we can do it because this is 2D)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "plt.figure(2, figsize=(8, 6))\n",
    "plt.clf()\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, edgecolor=\"k\")\n",
    "plt.xlabel(\"Sepal length\")\n",
    "plt.ylabel(\"Sepal width\")\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "\"\"\"In the plot above, the colors represent the 3 different classes.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris = datasets.load_iris()\n",
    "X_ = iris.data[:, :3]  # we only take the first three features for graphic purposes --> visualization in a 3-dimensional space.\n",
    "y = iris.target\n",
    "\n",
    "print(\"feature matrix size\", X_.shape)\n",
    "print(\"label vector size\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "\n",
    "ax = fig.add_subplot(111, projection=\"3d\", elev=48, azim=134)\n",
    "ax.set_position([0, 0, 0.95, 1])\n",
    "\n",
    "plt.cla()\n",
    "\n",
    "for name, label in [(\"Setosa\", 0), (\"Versicolour\", 1), (\"Virginica\", 2)]:\n",
    "    ax.text3D(\n",
    "        X_[y == label, 0].mean(),\n",
    "        X_[y == label, 1].mean() + 1.5,\n",
    "        X_[y == label, 2].mean(),\n",
    "        name,\n",
    "        horizontalalignment=\"center\",\n",
    "        bbox=dict(alpha=0.5, edgecolor=\"w\", facecolor=\"w\"),\n",
    "    )\n",
    "\n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y = np.choose(y, [1, 2, 0]).astype(float)\n",
    "ax.scatter(X_[:, 0], X_[:, 1], X_[:, 2], c=y, cmap=plt.cm.nipy_spectral, edgecolor=\"k\")\n",
    "\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_zticklabels([])\n",
    "\n",
    "ax.set_xlabel('Sepal length')\n",
    "ax.set_ylabel('Sepal width')\n",
    "ax.set_zlabel('Petal length')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is an unsupervised learning algorithm that tries to identify the subspace in which data approximately lies.\n",
    "\n",
    "Suppose we are given a dataset $\\{ x^{(i)}; i = 1, ... n\\}$ of $n$ features, where $x^{(i)} \\in \\mathbb{R}^d (d \\ll n)$. But unknown to us, two different features are almost linearly dependent, up to only small differences. We might say that the data lies approximately on an $n-1$ dimensional subspace.\n",
    "\n",
    "How can we automaticaly detect, and perhaps remove, this redudancy?\n",
    "\n",
    "If, for instance, we have just two features, $x_1$ and $x_2$ that are strongly correlated (linearly dependend), we might say that the data actually lies along some diagonal axis (the $u$ direction). How can we automatically compute this $u$ direction, the direction on which the data approximately lies?\n",
    "\n",
    "One way is to pose this problem as finding the unit vector $u$ so that when the data is projected onto the direction corresponding to $u$, the variance of the projected data is maximized. Intuitively, the data starts off with some amount of variance/information in it. We would like to choose a direction $u$ such that if we approximate the data as lying in the direction/subspace corresponding to $u$, as much s possible of this variance is still retained.\n",
    "\n",
    "Consider the following dataset \n",
    "\n",
    "<img src=\"img/dataset.png\" alt=\"Dataset Visualization\" width=\"500\">\n",
    "\n",
    "Now, suppose we pick u to correspond the the direction shown in this figure \n",
    "\n",
    "<img src=\"img/first_u.png\" alt=\"First Principal Component\" width=\"500\">\n",
    "\n",
    "The circles denote the projections of the original data onto this line.\n",
    "We see that the projected data still has a fairly large variance,and the points tend to be far from zero. \n",
    "In contrast, suppose had instead picked this other direction \n",
    "\n",
    "<img src=\"img/second_u.png\" alt=\"Second Principal Component\" width=\"500\">\n",
    "\n",
    "The projections have a significantly smaller variance, and are much\n",
    "closer to the origin.\n",
    "\n",
    "we would like to select the direction $u$ corresponding to the first of the two figures above.\n",
    "To formalize thism note that given a unit vector $u$ and a point $x$, the length of the projection of $x$ onto $u$ is given by the scalar product $x^Tu$, i.e. if $x^{(i)}$ is a point in our datasen, its progection onto $u$ is the distance $x^Tu$ from the origin. \n",
    "\n",
    "Hence, to maximize the variance of the projections, we would like to choose a unit-length $u$ so as to maximize:\n",
    "\\begin{equation*}\n",
    "\\frac{1}{n} \\sum_{i=1}^n ( x^{(i)^T} u )^2 = \\frac{1}{n} \\sum_{i=1}^n u^T x^{(i)} x^{(i)^T} u = u^T \\Big( \\frac{1}{n} \\sum_{i=1}^n x^{(i)} x^{(i)^T} \\Big) u \\ .\n",
    "\\end{equation*}\n",
    "Maximizing this, subject to $|| u ||_2 = 1$ gives the _principal eigenvector_ of $\\Sigma = \\frac{1}{n} \\Big( \\sum_{i=1}^n x^{(i)} x^{(i)^T} \\Big)$, which is just the empirical covariance matrix of the data.\n",
    "\n",
    "__To summarize__, we have found that if we wish to find a $1$-dimensional\n",
    "subspace with witch to approximate the data, we should choose $u$ to be the\n",
    "principal eigenvector of $\\Sigma$. More generally, if we wish to project our data\n",
    "into a $k$-dimensional subspace ($k < d$), we should choose $u_1 , . . . , u_k$ to be the\n",
    "top $k$ eigenvectors of $\\Sigma$. The u_i ’s now form a new, orthogonal basis for the\n",
    "data.\n",
    "Then, to represent $x^{(i)}$ in this basis, we need only compute the corresponding vector\n",
    "\n",
    "\\begin{equation}\n",
    "y^{(i)} = \\begin{pmatrix} \n",
    "u^T_1 x^{(i)} \\\\ u^T_2 x^{(i)} \\\\ ... \\\\ u^T_k x^{(i)} \n",
    "\\end{pmatrix} \n",
    "\\in \\mathbb{R}^k \\ .\n",
    "\\end{equation}\n",
    "\n",
    "Thus, the vector $y^{(i)}$ gives a lower, $k$-dimenisional, approximation/representation for $x^{(i)} \\in \\mathbb{R}^d$.\n",
    "\n",
    "PCA is also referred to as __dimensionality reduction__ algorithm. The vectors $u_1 , . . . , u_k$ are called the _first k_ __principal components__ of the data.\n",
    "\n",
    "A standard application f PCA is to preprocess a dataset to reduce its\n",
    "dimension before running a supervised learning learning algorithm with the\n",
    "$x^{(i)}$ ’s as inputs. Apart from computational benefits, reducing the data’s\n",
    "dimension can also reduce the complexity of the hypothesis class considered\n",
    "and help avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***PCA on the IRIS dataset***\n",
    "\n",
    "Principal component analysis (PCA) performs a linear transformation on the data so that most of the variance or information in your high-dimensional dataset is captured by the first few principal components.\n",
    "\n",
    "The first principal component will capture the most variance, followed by the second principal component, and so on.\n",
    "\n",
    "Each principal component is a linear combination of the original variables. Because all the principal components are orthogonal to each other, there is no redundant information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "np.random.seed(5)  #input seed value to generate repeated random value\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(\"feature matrix size\", X.shape)\n",
    "print(\"label vector size\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA #principal component analysis\n",
    "\n",
    "pca = PCA(n_components=3) # here I have to choose the number of PCA components\n",
    "pca.fit(X) # Fit the model with X\n",
    "\n",
    "X_red = pca.transform(X) # Apply dimensionality reduction to X\n",
    "\n",
    "print(X_red.shape)\n",
    "print(pca.explained_variance_ratio_) # Percentage of variance explained by each of the selected components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine explained variance using explained_variance_ration_ attribute\n",
    "#\n",
    "exp_var_pca = pca.explained_variance_ratio_\n",
    "#\n",
    "# Cumulative sum of eigenvalues; This will be used to create step plot\n",
    "# for visualizing the variance explained by each principal component.\n",
    "#\n",
    "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "print(cum_sum_eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the visualization plot\n",
    "#\n",
    "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many component do we want to save?\n",
    "\n",
    "pca = decomposition.PCA(n_components=3) #here I have to choose the number of PCA components\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = iris.target  # original labels\n",
    "y_vis = np.choose(y_true, [1, 2, 0]).astype(float)  # for coloring\n",
    "\n",
    "print(y_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "\n",
    "ax = fig.add_subplot(111, projection=\"3d\", elev=48, azim=134)\n",
    "ax.set_position([0, 0, 0.95, 1])\n",
    "\n",
    "# Make sure y_true is integer (this is the key fix!)\n",
    "y_true = y.astype(int)  # Ensure it's int for indexing and np.choose\n",
    "\n",
    "# Then use y_true in indexing:\n",
    "for name, label in [(\"Setosa\", 0), (\"Versicolour\", 1), (\"Virginica\", 2)]:\n",
    "    ax.text3D(\n",
    "        X[y_true == label, 0].mean(),\n",
    "        X[y_true == label, 1].mean() + 1.5,\n",
    "        X[y_true == label, 2].mean(),\n",
    "        name,\n",
    "        horizontalalignment=\"center\",\n",
    "        bbox=dict(alpha=0.5, edgecolor=\"w\", facecolor=\"w\"),\n",
    "    )\n",
    "\n",
    "# Use y_vis for coloring\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y_vis, cmap=plt.cm.nipy_spectral, edgecolor=\"k\")\n",
    "\n",
    "# Hide tick labels\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_zticklabels([])\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel('Sepal length')\n",
    "ax.set_ylabel('Sepal width')\n",
    "ax.set_zlabel('Petal length')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "\n",
    "ax = fig.add_subplot(111, projection=\"3d\", elev=48, azim=134)\n",
    "ax.set_position([0, 0, 0.95, 1])\n",
    "\n",
    "# Make sure y_true is integer (this is the key fix!)\n",
    "y_true = y.astype(int)  # Ensure it's int for indexing and np.choose\n",
    "\n",
    "# Add class labels at the mean position\n",
    "for name, label in [(\"Setosa\", 0), (\"Versicolour\", 1), (\"Virginica\", 2)]:\n",
    "    ax.text3D(\n",
    "        X[y_true == label, 0].mean(),\n",
    "        X[y_true == label, 1].mean() + 1.5,\n",
    "        X[y_true == label, 2].mean(),\n",
    "        name,\n",
    "        horizontalalignment=\"center\",\n",
    "        bbox=dict(alpha=0.5, edgecolor=\"w\", facecolor=\"w\"),\n",
    "    )\n",
    "\n",
    "# Reorder labels just for color visualization\n",
    "y_vis = np.choose(y_true, [1, 2, 0]).astype(float)\n",
    "\n",
    "# Scatter plot\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y_vis, cmap=plt.cm.nipy_spectral, edgecolor=\"k\")\n",
    "\n",
    "# Remove tick labels\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_zticklabels([])\n",
    "\n",
    "# Axis labels\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
