{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "import cv2\n",
    "\n",
    "# from google.colab.patches import cv2_imshow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the iris dataset\n",
    "df = pd.read_csv('iris.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe of feature\n",
    "df_feature=df.loc[:, [\"sepal.length\",\t\"sepal.width\", \"petal.length\",\"petal.width\"]]\n",
    "\n",
    "#df_feature.head()\n",
    "\n",
    "#dataframe of label\n",
    "df_label=df.loc[:, [\"variety\"]]\n",
    "\n",
    "#df_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataframe of feature and a dataframe of label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Hyperparameters tuning\n",
    "\n",
    "*  Splitting training/testing\n",
    "*  Freezing the test set\n",
    "*  Spitting training/validation (to retrieve a validation set for hyperparameters tuning)\n",
    "*  Defining hyperparameters to be tuned (in this case 2 and 2 hyperparameters)\n",
    "*  Initializing 4 classifiers with the 4 combination of hyperparameters\n",
    "*  Training the 4 classifiers\n",
    "*  Testing the 4 classifiers performance on the validation set\n",
    "*  Choosing the best combination of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split (10% of the training dataset will be used for testing)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_feature, df_label, test_size = .10)\n",
    "\n",
    "print(\"original set len\", len(df_feature))\n",
    "print(\"training set len\", len(x_train))\n",
    "print(\"test set len\", len(x_test))\n",
    "\n",
    "# Train/Test split (20% of the new training dataset will be used for validation)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=.20)\n",
    "\n",
    "print(\"training set new len\", len(x_train))\n",
    "print(\"valid set len\", len(x_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the different classifiers (i.e., the same classifier with different combinantion of hyperparameters).\n",
    "* n_estimators = number of decison trees (typically between 10 and 1000).\n",
    "* max_depth = defines the maximum depth of the tree, i.e. the number of decision nodes in each decision tree. If `None`, then nodes are expanded until all leaves are pure.\n",
    "\n",
    "We want to tune the following hyperparameters:\n",
    "* n_estimators = 100,200\n",
    "* max_depth = 2,3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = RandomForestClassifier(n_estimators = 100, max_depth = 2)\n",
    "clf2 = RandomForestClassifier(n_estimators = 100, max_depth = 3)\n",
    "clf3 = RandomForestClassifier(n_estimators = 200, max_depth = 2)\n",
    "clf4 = RandomForestClassifier(n_estimators = 200, max_depth = 3)\n",
    "\n",
    "trainedclf1=clf1.fit(x_train, y_train)\n",
    "trainedclf2=clf2.fit(x_train, y_train)\n",
    "trainedclf3=clf3.fit(x_train, y_train)\n",
    "trainedclf4=clf4.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validating the best classifier (i.e., the one with the best combination of hyperparameters (i.e., the one with the highest performance on the validation set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction1=trainedclf1.predict(x_valid)\n",
    "prediction2=trainedclf2.predict(x_valid)\n",
    "prediction3=trainedclf3.predict(x_valid)\n",
    "prediction4=trainedclf4.predict(x_valid)\n",
    "\n",
    "print(accuracy_score(y_valid, prediction1))\n",
    "print(accuracy_score(y_valid, prediction2))\n",
    "print(accuracy_score(y_valid, prediction3))\n",
    "print(accuracy_score(y_valid, prediction4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally test the performance of the best classifier after hyperparameters tuning on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "predictiontestset=trainedclf1.predict(x_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "print(accuracy_score(y_test, predictiontestset))\n",
    "\n",
    "#confusion matrix\n",
    "classes=[\"Setosa\", \"Virginica\", \"Versicolor\"]\n",
    "cm = confusion_matrix(y_test, predictiontestset, labels=classes)\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=classes)\n",
    "\n",
    "cmd.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning: a more complex but accepted scenario\n",
    "\n",
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data.\n",
    "\n",
    "This situation is called **overfitting**.\n",
    "\n",
    "To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test.\n",
    "\n",
    "\n",
    "When evaluating different settings (“hyperparameters”) for estimators, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally.\n",
    "\n",
    "This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance.\n",
    "\n",
    "To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
    "\n",
    "However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n",
    "\n",
    "A solution to this problem is **cross-validation**. A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets.\n",
    "\n",
    "The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "- A model is trained using k-1 of the folds as training data;\n",
    "\n",
    "- the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the cross-validation procedure\n",
    "loop = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "# define the model\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# define search space\n",
    "grid = dict()\n",
    "grid['n_estimators'] = [10, 100, 500]\n",
    "grid['max_depth'] = [2, 4, 6]\n",
    "\n",
    "# define search for hyperparameters tuning\n",
    "search = GridSearchCV(clf, grid, scoring='accuracy', cv=loop)\n",
    "\n",
    "# execute search\n",
    "result = search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best performing model fit on the whole training dataset (i.e., the one with the best hyperparameters)\n",
    "best_model = result.best_estimator_\n",
    "print(\"the best model is \", best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on the test set\n",
    "predictions=best_model.predict(x_test)\n",
    "print(\"the accuracy is \", accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A global overview of cross validation\n",
    "search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print keys of interest for viewing hyperparamenters.\n",
    "print(search.cv_results_['params'])\n",
    "print(search.cv_results_['mean_test_score']) #mean of performance of each fold to discriminate hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
