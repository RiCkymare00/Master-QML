{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "\n",
    "SVM is a supervised learning algorithm which allows to find a __hyperplane__ (or a set of hyperplanes) in order to classify data in a high (or infinite) dimensional features spaces. \n",
    "Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training data points of any class (so-called _functional margin_), since in general the larger the margin the lower the generalization error of the classifier.\n",
    "It would be nice if, given a training set, we were able to finh a _decision boundary_ that allows us to make all correct and confident (meaning far from the decision boundary) predictions on the training examples.\n",
    "\n",
    "The figure below shows the decision bondary for a linearly separable problem, with three samples on the margin boundaries, called __support vectors__\n",
    "\n",
    "<img src=\"img/decision_boundary.png\" alt=\"Decision Boundary Visualization\" width=\"500\">\n",
    "\n",
    "The number of __support vectors__ can be much smaller than the size of the training set.\n",
    "\n",
    "## Functional and geometric margins\n",
    "\n",
    "Given a training example $(x^{(i)} , y^{(i)} )$, we define the __functional margin__ of $(w, b)$ with respect to the training example as\n",
    "\\begin{equation*}\n",
    "\\hat{\\gamma}^{(i)} = y^{(i)} (w^T x^{(i)} + b) \\ .\n",
    "\\end{equation*}\n",
    "\n",
    "Note that:\n",
    "* if $y^{(i)}=1$, then for the functional margin to be large (i.e., for our prediction to be confident and correct), we need $w^T x^{(i)} + b$ to be a large positive number\n",
    "* if $y^{(i)}=-1$, then for the functional margin to be large (i.e., for our prediction to be confident and correct), we need $w^T x^{(i)} + b$ to be a large negative number\n",
    "* if $y^{(i)} (w^T x^{(i)} + b) > 0$, then our prediction on this example is correct.\n",
    "\n",
    "Hence, a large functional margin represents a confident and a correct prediction. There’s one property of the functional margin that makes it not a very good measure of confidence: it is not invariant under rescaling of the parameters $(w,b)$.\n",
    "\n",
    "In order to overcome this problem we define the __geometric margin__ of $(w,b)$ with respect to a training example $(x^{(i)} , y^{(i)} )$ as\n",
    "\\begin{equation*}\n",
    "\\gamma^{(i)} = y^{(i)} \\left( \\frac{w^T}{||w||} x^{(i)} + \\frac{b}{||w||} \\right) \\ .\n",
    "\\end{equation*}\n",
    "\n",
    "The geometric margin is invariant under rescaling of the parameters and if $||w||=1$ geometric margin equals functional margin.\n",
    "\n",
    "## The optimal margin classifier\n",
    "\n",
    "Given a training set, it seems that a natural way to find the best decision boundary is to maximize the geometric margins, since this would reflect a very confident set of predictions on the training set and a good “fit” to the training data. \n",
    "Specifically, this will result in a classifier that separates the positive and the negative training examples with a high level of confidence.\n",
    "\n",
    "Assuming that our dataset is linearly separable, meanig that it is always possible to separate the positive and negative examples using some separating hyperplane, one has to solve the following optimization problem\n",
    "\\begin{align*}\n",
    "max& _{\\gamma, w, b}  \\quad \\gamma \\\\\n",
    "& s.t. \\quad y^{(i)} (w^T x^{(i)} + b) \\geq 1 \\ ,\\  i = 1, ..., n \\\\\n",
    "& || w || = 1 \\ .\n",
    "\\end{align*}\n",
    "\n",
    "I.e., we want to maximize $\\gamma$, subject to each training example having functional margin at least $\\gamma$. The $||w|| = 1$ constraint moreover ensures that the\n",
    "functional margin equals to the geometric margin, so we are also guaranteed\n",
    "that all the geometric margins are at least $\\gamma$. Thus, solving this problem will\n",
    "result in $(w, b)$ with the largest possible geometric margin with respect to the\n",
    "training set. The problem here is that the $||w|| = 1$ constraint is non convex. \n",
    "\n",
    "One can show that the above maximization problem can be reformulated in the following dual minimization problem\n",
    "\\begin{align*}\n",
    "min&_{\\gamma, w, b}  \\quad \\frac{1}{2} || w ||^2 \\\\\n",
    "& s.t. \\quad y^{(i)} (w^T x^{(i)} + b) \\geq 1 \\ ,\\  i = 1, ..., n \\ .\n",
    "\\end{align*}\n",
    "\n",
    "In this form it is an optimization problem with a convex quadratic objective function and only linear constraints. Its solution gives us the __optimal margin classifier__.\n",
    "\n",
    "The solution of the optimization problem allows to find the optimal value of $w$ as\n",
    "<a id=\"optimal_w\"></a>\n",
    "\\begin{equation} \n",
    "w = \\sum_{i=1}^n \\alpha_i y^{(i)} x^{(i)} \\ , \n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "where the $\\alpha_i$ are called _dual coefficients_ (they arise as Langrange multipliers) and are __non zero only for the support vectors__.\n",
    "\n",
    "Once the optimization problem is solved, and we have found the optimal $w$, we can make a prediction for a given new sample x. We would then calculate $w^T x + b$ and predict $y=1$ if and only if this quantity is bigger than zero. This quantity can be written as\n",
    "\n",
    "\\begin{align} \n",
    "w^T x + b & = \\Big( \\sum_{i=1}^n \\alpha_i y^{(i)} x^{(i)} \\Big)^T x + b \\\\\n",
    "& = \\sum_{i=1}^n \\alpha_i y^{(i)} x^{(i)^T} x + b \\\\\n",
    "& = \\sum_{i=1}^n \\alpha_i y^{(i)} \\langle x^{(i)}, x \\rangle + b \\\\\n",
    "& = \\sum_{i=1}^n \\alpha_i y^{(i)} K(x^{(i)}, x) + b \\ ,\n",
    "\\end{align} \\tag{2}\n",
    "where $K(\\cdot, \\cdot)$ is the (linear) _kernel_.\n",
    "\n",
    "This results shows that if we’ve found the $\\alpha_i$’s, in order to make a prediction, we have to\n",
    "calculate a quantity that depends only on the inner product between $x$ and\n",
    "the points in the training set $x^{(i)}$. Moreover, the $\\alpha_i$’s will all\n",
    "be zero except for the support vector, and thus, many of the terms in the sum\n",
    "above will be zero, and we need to find only the inner products between\n",
    "$x$ and the support vectors (of which there is often only a small number) in\n",
    "order to make our prediction.\n",
    "\n",
    "## Regularization and the non-separable case\n",
    "\n",
    "The derivation of the SVM as presented so far assumed that the data is linearly separable. While mapping data to a high dimensional feature space\n",
    "via $\\phi$ does generally increase the likelihood that the data is separable, we can’t guarantee that it always will be so.\n",
    "\n",
    "To make the algorithm work for non-linearly separable datasets (as well as be less sensitive to outliers), we reformulate our optimization (using $l_1$ regularization) as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "min&_{\\gamma, w, b}  \\quad \\frac{1}{2} || w ||^2 + C \\sum_{i=1}^n  \\xi_i \\\\\n",
    "& s.t. \\quad y^{(i)} (w^T x^{(i)} + b) \\geq 1 - \\xi_i \\ ,\\  i = 1, ..., n \\\\\n",
    "& \\qquad \\  \\xi_i \\geq 0, i = 1, ..., n \\ .\n",
    "\\end{align*}\n",
    "\n",
    "In this formulation, we allow some samples to be at a distance less than $1$ from their correct functional margin.\n",
    "If an example has functional margin $1 − \\xi_i$ (with $\\xi_i$ > 0), we would pay a penalty to the objective function being increased by $C\\xi_i$.\n",
    "The term $C$ controls the strength of this penalty, and as a result, acts as an inverse regularization parameter.\n",
    "\n",
    "As before, we also have that $w$ can be expressed in terms of the $\\alpha_i$’s as\n",
    "given in equation (1), so that after solving the dual problem, we can continue to use equation (2) to make our predictions.\n",
    "\n",
    "Note that, somewhat surprisingly, in adding $l_1$ regularization, the only change to the dual problem is that what was originally a constraint that $\\alpha_i \\geq 0$ has now become $0 \\leq \\alpha_i \\leq C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #library that contains multidimensional array and matrix data structures\n",
    "import pandas as pd #library useful for data structures and data analysis\n",
    "import seaborn as sns # data visualization library based on matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "from collections import Counter\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('iris.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separete the features and target\n",
    "\n",
    "X = df_feature=df.loc[:, [\"sepal.length\",\t\"sepal.width\", \"petal.length\",\"petal.width\"]]\n",
    "Y = df.loc[:, [\"variety\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to train and test dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split # split arrays or matrices into random train and test subsets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "# test_size, if given in float (should be between 0.0 and 1.0), represents the proportion of the dataset to include in the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support vector machine algorithm\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear', C=1, probability=True)\n",
    "# kernel: specifies the kernel type to be used in the algorithm\n",
    "# C: regularization parameter. The strength of the regularization is inversely proportional to C.\n",
    "# probability: whether to enable probability estimates\n",
    "\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us access the parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"kernel: \",svm.kernel) # print the kernel used\n",
    "print(\"dual coefficients:\\n\",svm.dual_coef_) # print the dual coefficients, i.e, the products y_i * alpha_i\n",
    "print(\"support vectors:\\n\",svm.support_vectors_) # print the support vectors\n",
    "print(\"number of support vectors:\\n\",len(svm.support_vectors_)) # print the total number of support vectors\n",
    "print(\"number of support vectors for each class:\\n\",svm.n_support_) # print the number of support vectors for each class\n",
    "print(\"number of support vectors:\\n\",svm.n_support_.sum()) # print the total number of support vectors\n",
    "print(\"intercept:\\n\",svm.intercept_) # print the intercept\n",
    "print(\"w: \\n\",svm.coef_) # print the weight vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can sees, the number of support vectors is $26$ which is much smaller than the number if training example, that is $120$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict from the test dataset\n",
    "predictedProbability = svm.predict_proba(X_test)\n",
    "print(\"Distribution of probability of a sample to belong to a class\\n\", predictedProbability) #predicted-probability for each sample to belong to a class\n",
    "\n",
    "predictions = svm.predict(X_test)\n",
    "print(\"vector of top predictions\\n\",predictions) #label-predicted\n",
    "\n",
    "print(\"test set\\n\",y_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Metrics\n",
    "\n",
    "## Confusion matrix\n",
    "\n",
    "A confusion matrix is a way to evaluate the accuracy of a classification.\n",
    "\n",
    "<img src=\"img/confusion_matrix.png\" alt=\"Confusion Matrix\" width=\"600\">\n",
    "\n",
    "__Precision__ and __Recall__ have to be defined for each class.\n",
    "\n",
    "* __Accuracy__: how often the model is correct globally. It is good when classes are balanced. In imbalanced datasets it can be misleading. \n",
    "* __Recall__ (Sensitivity/ True Positive Rate (TPR)): out of all actual positive, how many were correctly identified. High Recall = few false negatives.\n",
    "* __Precision__: of all predicted positives, how many were true. High Precision = few false positives.\n",
    "* __Specificity__ (True Negative Rate (TNR))= $\\frac{TN}{TN + FP}$. High Specificity = few false positives.\n",
    "* __F1 Score__: $F1 = 2 \\frac{Precision \\cdot Recall}{Precision + Recall}$. It describes the harmonic mean between precision and recall. It favors balance and is useful when the dataset is imbalanced or both false positive and false negative matter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "classes=[\"Setosa\", \"Virginica\", \"Versicolor\"]\n",
    "cm = confusion_matrix(y_test, predictions, labels=classes)\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=classes)\n",
    "cmd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic (ROC) Curve & AUC (Area Under Curve)\n",
    "\n",
    "* The __ROC curve__ is a graphical representation of a classification model's ability to distinguish between positive and negative classes across different threshold values. It plots the __True Positive Rate (TPR or Recall)__ vs. __False Positive Rate__ (FPR = FP/(FP + TN)) at different thresholds.\n",
    "\n",
    "* The __AUC__ measures a model's ability to distinguish between positive and negative classes. In particulat it gives the probability that the classifier ranks a randomly chosen positive higher than a randomly chosen negative.\n",
    "\n",
    "Interpretation:\n",
    "* AUC = 1.0: perfect separation of classes -> Perfect Classifier (TPR=1, FPR=0),, the curve reaches the top-left\n",
    "* AUC = 0.5: random guessing -> diagonal line\n",
    "* AUC < 0.5: curve below the diagonal -> bad classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer # binarize labels in a one-vs-all fashion\n",
    "\n",
    "label_binarizer = LabelBinarizer().fit(y_train)\n",
    "y_onehot_test = label_binarizer.transform(y_test)\n",
    "y_onehot_test.shape  # (n_samples, n_classes)\n",
    "\n",
    "# predictedProbability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_onehot_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score # compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "\n",
    "micro_roc_auc_ovr = roc_auc_score(\n",
    "    y_onehot_test,\n",
    "    predictedProbability,\n",
    "    multi_class=\"ovr\",\n",
    "    average=\"micro\",\n",
    ")\n",
    "\n",
    "print(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{micro_roc_auc_ovr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc # compute Receiver operating characteristic (ROC)\n",
    "fpr, tpr, roc_auc = dict(), dict(), dict()\n",
    "\n",
    "n_classes = 3\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_onehot_test[:, i], predictedProbability[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "fpr_grid = np.linspace(0.0, 1.0, 1000)\n",
    "\n",
    "# Interpolate all ROC curves at these points\n",
    "mean_tpr = np.zeros_like(fpr_grid)\n",
    "\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(fpr_grid, fpr[i], tpr[i])  # linear interpolation\n",
    "\n",
    "# Average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = fpr_grid\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "print(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['macro']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "iris = load_iris()\n",
    "target_names = iris.target_names\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "'''plt.plot(\n",
    "    fpr[\"micro\"],\n",
    "    tpr[\"micro\"],\n",
    "    label=f\"micro-average ROC curve (AUC = {roc_auc['micro']:.2f})\",\n",
    "    color=\"deeppink\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=4,\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    fpr[\"macro\"],\n",
    "    tpr[\"macro\"],\n",
    "    label=f\"macro-average ROC curve (AUC = {roc_auc['macro']:.2f})\",\n",
    "    color=\"navy\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=4,\n",
    ")'''\n",
    "\n",
    "colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n",
    "for class_id, color in zip(range(n_classes), colors):\n",
    "    RocCurveDisplay.from_predictions(\n",
    "        y_onehot_test[:, class_id],\n",
    "        predictedProbability[:, class_id],\n",
    "        name=f\"ROC curve for {target_names[class_id]}\",\n",
    "        color=color,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"ROC curve for chance level (AUC = 0.5)\")\n",
    "plt.axis(\"square\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Extension of Receiver Operating Characteristic\\nto One-vs-Rest multiclass\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
